import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict
from sklearn.preprocessing import OneHotEncoder
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from imblearn.pipeline import Pipeline as ImbPipeline
from imblearn.over_sampling import BorderlineSMOTE
from datetime import datetime

# --- Recreate your setup for demonstration ---
# (You would replace this with your actual df, X, y, identifiers, and pipeline training)
num_rows = 1000
df = pd.DataFrame({
    'ADOBE_ID': [f'ID_{i}' for i in range(num_rows)],
    'CUSTOMER_NAME': [f'Customer_{i}' for i in range(num_rows)],
    'EMAIL_ID': [f'email_{i}@example.com' for i in range(num_rows)],
    'MOBILE_NUMBER': [f'987654321{i % 10}' for i in range(num_rows)],
    'ORIGINAL_STATE': np.random.choice(['CA', 'NY', 'TX', 'FL'], num_rows),
    'LEAD_CREATED_DATE': pd.to_datetime('2023-01-01') + pd.to_timedelta(np.random.randint(0, 365, num_rows), unit='D'),
    'Insert Datetime (UTC)': [datetime.utcnow() for _ in range(num_rows)],
    'UNIQUE_PAGES': np.random.randint(1, 20, num_rows), # Simulating this
    'MAX_TIME_SPENT_PAGE_URL_CATEGORY': np.random.choice(['A', 'B', 'C', 'D'], num_rows), # Simulating this
    'OTHER_FEATURE_NUM': np.random.rand(num_rows) * 10,
    'TARGET': np.random.randint(0, 2, num_rows)
})

# Add an example where UNIQUE_PAGES is treated as a category or has high cardinality to force OHE
df['UNIQUE_PAGES_category'] = pd.cut(df['UNIQUE_PAGES'], bins=[0, 5, 10, 15, 20], labels=['low', 'medium', 'high', 'very_high'])
df = df.drop('UNIQUE_PAGES', axis=1) # Drop original if categorized


columns_to_drop = [
    'ADOBE_ID', 'CUSTOMER_NAME', 'EMAIL_ID', 'MOBILE_NUMBER',
    'ORIGINAL_STATE', 'LEAD_CREATED_DATE', 'Insert Datetime (UTC)',
    'TARGET'
]
X = df.drop(columns_to_drop, axis=1)
y = df['TARGET']

# Define categorical and numerical features for the preprocessor
categorical_features = X.select_dtypes(include='category').columns.tolist() + X.select_dtypes(include='object').columns.tolist()
numerical_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()

# Create preprocessor
preprocessor = ColumnTransformer(
    transformers=[
        ('num', 'passthrough', numerical_features),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
    ])

# Your model pipeline using ImbPipeline
gb_borderlinesmote_model = ImbPipeline([
    ('preprocessor', preprocessor), # Added preprocessor to the pipeline
    ('sampling', BorderlineSMOTE(random_state=42)),
    ('classifier', GradientBoostingClassifier(
        n_estimators=100,
        learning_rate=0.1,
        max_depth=6,
        min_samples_split=5,
        min_samples_leaf=2,
        random_state=42
    ))
])

# Train your model (using your X_train, y_train after split)
# For this example, let's just use X and y
from sklearn.model_selection import train_test_split
X_train, X_test_val, y_train, y_test_val = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)
X_val, X_test, y_val, y_test = train_test_split(X_test_val, y_test_val, test_size=0.666, random_state=42, stratify=y_test_val) # roughly 10/20 split of original


gb_borderlinesmote_model.fit(X_train, y_train)

# --- Assuming you have a way to get the trained model and feature names ---
# In your setup, this is part of results_df.
# For this example, we directly access it from the pipeline.
trained_classifier = gb_borderlinesmote_model.named_steps['classifier']

# Get feature names after preprocessing
# This requires fitting the preprocessor first to get feature names out
# Or, if you use the full pipeline.named_steps['preprocessor'], you can call .get_feature_names_out() on it.
# Let's get them explicitly.
preprocessor_fitted = gb_borderlinesmote_model.named_steps['preprocessor']
# Get feature names after one-hot encoding
feature_names_encoded = (
    list(numerical_features) +
    list(preprocessor_fitted.named_transformers_['cat'].get_feature_names_out(categorical_features))
)

# 1. Get feature importances from the trained classifier
importances = trained_classifier.feature_importances_

# Create a DataFrame for importances
feat_df = pd.DataFrame({
    'Feature': feature_names_encoded,
    'Importance': importances
})

# 2. Group by original feature using prefix before the first underscore
# This handles the one-hot encoded features
grouped_importance = defaultdict(float)

for feature, importance in zip(feat_df['Feature'], feat_df['Importance']):
    # Special handling for your specific prefixes
    if 'UNIQUE_PAGES_category' in feature:
        base_feature = 'UNIQUE_PAGES_category' # Or 'UNIQUE_PAGES' if you prefer
        grouped_importance[base_feature] += importance
    elif 'MAX_TIME_SPENT_PAGE_URL_CATEGORY' in feature:
        base_feature = 'MAX_TIME_SPENT_PAGE_URL_CATEGORY'
        grouped_importance[base_feature] += importance
    elif '_' in feature and feature.split('_')[0] in categorical_features: # General OHE case
        base_feature = feature.split('_')[0]
        grouped_importance[base_feature] += importance
    else: # Numerical features or features not one-hot encoded
        grouped_importance[feature] += importance

# Convert grouped importances to a DataFrame
global_contributions_df = pd.DataFrame(
    list(grouped_importance.items()),
    columns=['Feature', 'Importance']
).sort_values(by='Importance', ascending=False)

# Normalize to percentage
total_importance = global_contributions_df['Importance'].sum()
global_contributions_df['Percentage'] = (global_contributions_df['Importance'] / total_importance) * 100

print("Global Feature Contributions:")
print(global_contributions_df)

# 3. Plotting the Global Variable Contribution
plt.figure(figsize=(12, 8))
sns.barplot(x='Percentage', y='Feature', data=global_contributions_df, palette='viridis')
plt.title('Global Feature Contributions (Percentage)')
plt.xlabel('Contribution Percentage (%)')
plt.ylabel('Feature')

# Add percentage labels on the bars
for index, row in global_contributions_df.iterrows():
    plt.text(row.Percentage + 0.5, index, f'{row.Percentage:.1f}%', va='center')

plt.xlim(0, global_contributions_df['Percentage'].max() * 1.1) # Adjust x-lim for labels
plt.tight_layout()
plt.show()

